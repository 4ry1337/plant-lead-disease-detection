{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7e1b94-e3d4-4106-8db2-8b9d4a5315fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from functools import lru_cache\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from einops import rearrange\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.layers import DropPath, to_2tuple\n",
    "from torch import Tensor\n",
    "from torch.types import _size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7e69b3c-debd-4837-b69c-b72d595d0bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "        dilation: int = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_3 = nn.Conv2d(\n",
    "            in_channels // 2,\n",
    "            out_channels // 2,\n",
    "            kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups // 2,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.conv_5 = nn.Conv2d(\n",
    "            in_channels - in_channels // 2,\n",
    "            out_channels - out_channels // 2,\n",
    "            kernel_size + 2,\n",
    "            stride=stride,\n",
    "            padding=padding + 1,\n",
    "            dilation=dilation,\n",
    "            groups=groups - groups // 2,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x1, x2 = x.chunk(2, dim=1)\n",
    "        x1 = self.conv_3(x1)\n",
    "        x2 = self.conv_5(x2)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DES(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        act_func: nn.Module = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        _, self.p = self._decompose(min(in_features, out_features))\n",
    "        self.k_out = out_features // self.p\n",
    "        self.k_in = in_features // self.p\n",
    "        self.proj_right = nn.Linear(self.p, self.p, bias=bias)\n",
    "        self.act = act_func()\n",
    "        self.proj_left = nn.Linear(self.k_in, self.k_out, bias=bias)\n",
    "\n",
    "    def _decompose(self, n: int) -> List[int]:\n",
    "        assert n % 2 == 0, f\"Feature dimension has to be a multiple of 2, but got {n}\"\n",
    "        e = int(math.log2(n))\n",
    "        e1 = e // 2\n",
    "        e2 = e - e // 2\n",
    "        return 2 ** e1, 2 ** e2\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        B = x.shape[:-1]\n",
    "        x = x.view(*B, self.k_in, self.p)\n",
    "        x = self.proj_right(x).transpose(-1, -2)\n",
    "        if self.act is not None:\n",
    "            x = self.act(x)\n",
    "        x = self.proj_left(x).transpose(-1, -2).flatten(-2, -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MixCFN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: Optional[int] = None,\n",
    "        out_features: Optional[int] = None,\n",
    "        act_func: nn.Module = nn.GELU,\n",
    "        with_cp: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.with_cp = with_cp\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.conv = MixConv2d(\n",
    "            hidden_features,\n",
    "            hidden_features,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            groups=hidden_features,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.act = act_func()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x: Tensor, H: int, W: int) -> Tensor:\n",
    "        def _inner_forward(x: Tensor) -> Tensor:\n",
    "            x = self.fc1(x)\n",
    "            B, N, C = x.shape\n",
    "            x = self.conv(x.transpose(1, 2).view(B, C, H, W))\n",
    "            x = self.act(x)\n",
    "            x = self.fc2(x.flatten(2).transpose(-1, -2))\n",
    "            return x\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            x = cp.checkpoint(_inner_forward, x)\n",
    "        else:\n",
    "            x = _inner_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a04556-bfbb-46a8-8b88-f223e3afd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRViTAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int = 64,\n",
    "        dim: int = 64,\n",
    "        heads: int = 2,\n",
    "        ws: int = 1,  # window size\n",
    "        qk_scale: Optional[float] = None,\n",
    "        proj_drop: float = 0.0,\n",
    "        with_cp: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % heads == 0, f\"dim {dim} should be divided by num_heads {heads}.\"\n",
    "        self.in_dim = in_dim\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim // heads\n",
    "        self.ws = ws\n",
    "        self.with_cp = with_cp\n",
    "\n",
    "        self.to_qkv = nn.Linear(in_dim, 2 * dim)\n",
    "\n",
    "        self.scale = qk_scale or self.dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.attn_act = nn.Hardswish(inplace=True)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Dropout(proj_drop),\n",
    "        )\n",
    "\n",
    "        self.attn_bn = nn.BatchNorm1d(\n",
    "            dim, momentum=1 - 0.9 ** 0.5 if self.with_cp else 0.1\n",
    "        )\n",
    "        nn.init.constant_(self.attn_bn.bias, 0)\n",
    "        nn.init.constant_(self.attn_bn.weight, 0)\n",
    "\n",
    "        self.parallel_conv = nn.Sequential(\n",
    "            nn.Hardswish(inplace=False),\n",
    "            nn.Conv2d(\n",
    "                dim,\n",
    "                dim,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                groups=dim,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    @lru_cache(maxsize=4)\n",
    "    def _generate_attn_mask(self, h: int, hp: int, device):\n",
    "        x = torch.empty(hp, hp, device=device).fill_(-100.0)\n",
    "        x[:h, :h] = 0\n",
    "        return x\n",
    "\n",
    "    def _cross_shaped_attention(\n",
    "        self,\n",
    "        q: Tensor,\n",
    "        k: Tensor,\n",
    "        v: Tensor,\n",
    "        H: int,\n",
    "        W: int,\n",
    "        HP: int,\n",
    "        WP: int,\n",
    "        ws: int,\n",
    "        horizontal: bool = True,\n",
    "    ):\n",
    "        B, N, C = q.shape\n",
    "        if C < self.dim_head:  # half channels are smaller than the defined dim_head\n",
    "            dim_head = C\n",
    "            scale = dim_head ** -0.5\n",
    "        else:\n",
    "            scale = self.scale\n",
    "            dim_head = self.dim_head\n",
    "\n",
    "        if horizontal:\n",
    "            q, k, v = map(\n",
    "                lambda y: y.reshape(B, HP // ws, ws, W, C // dim_head, -1)\n",
    "                .permute(0, 1, 4, 2, 3, 5)\n",
    "                .flatten(3, 4),\n",
    "                (q, k, v),\n",
    "            )\n",
    "        else:\n",
    "            q, k, v = map(\n",
    "                lambda y: y.reshape(B, H, WP // ws, ws, C // dim_head, -1)\n",
    "                .permute(0, 2, 4, 3, 1, 5)\n",
    "                .flatten(3, 4),\n",
    "                (q, k, v),\n",
    "            )\n",
    "\n",
    "        attn = q.matmul(k.transpose(-2, -1)).mul(\n",
    "            scale\n",
    "        )  # [B,H_2//ws,W_2//ws,h,(b1*b2+1)*(ws*ws),(b1*b2+1)*(ws*ws)]\n",
    "\n",
    "        ## need to mask zero padding before softmax\n",
    "        if horizontal and HP != H:\n",
    "            attn_pad = attn[:, -1:]  # [B, 1, num_heads, ws*W, ws*W]\n",
    "            mask = self._generate_attn_mask(\n",
    "                h=(ws - HP + H) * W, hp=attn.size(-2), device=attn.device\n",
    "            )  # [ws*W, ws*W]\n",
    "            attn_pad = attn_pad + mask\n",
    "            attn = torch.cat([attn[:, :-1], attn_pad], dim=1)\n",
    "\n",
    "        if not horizontal and WP != W:\n",
    "            attn_pad = attn[:, -1:]  # [B, 1, num_head, ws*H, ws*H]\n",
    "            mask = self._generate_attn_mask(\n",
    "                h=(ws - WP + W) * H, hp=attn.size(-2), device=attn.device\n",
    "            )  # [ws*H, ws*H]\n",
    "            attn_pad = attn_pad + mask\n",
    "            attn = torch.cat([attn[:, :-1], attn_pad], dim=1)\n",
    "\n",
    "        attn = self.attend(attn)\n",
    "\n",
    "        attn = attn.matmul(v)  # [B,H_2//ws,W_2//ws,h,(b1*b2+1)*(ws*ws),D//h]\n",
    "\n",
    "        attn = rearrange(\n",
    "            attn,\n",
    "            \"B H h (b W) d -> B (H b) W (h d)\"\n",
    "            if horizontal\n",
    "            else \"B W h (b H) d -> B H (W b) (h d)\",\n",
    "            b=ws,\n",
    "        )  # [B,H_1, W_1,D]\n",
    "        if horizontal and HP != H:\n",
    "            attn = attn[:, :H, ...]\n",
    "        if not horizontal and WP != W:\n",
    "            attn = attn[:, :, :W, ...]\n",
    "        attn = attn.flatten(1, 2)\n",
    "        return attn\n",
    "\n",
    "    def forward(self, x: Tensor, H: int, W: int) -> Tensor:\n",
    "        def _inner_forward(x: Tensor) -> Tensor:\n",
    "            B = x.shape[0]\n",
    "            ws = self.ws\n",
    "            qv = self.to_qkv(x)\n",
    "            q, v = qv.chunk(2, dim=-1)\n",
    "\n",
    "            v_conv = (\n",
    "                self.parallel_conv(v.reshape(B, H, W, -1).permute(0, 3, 1, 2))\n",
    "                .flatten(2)\n",
    "                .transpose(-1, -2)\n",
    "            )\n",
    "\n",
    "            qh, qv = q.chunk(2, dim=-1)\n",
    "            vh, vv = v.chunk(2, dim=-1)\n",
    "            kh, kv = vh, vv  # share key and value\n",
    "\n",
    "            # padding to a multple of window size\n",
    "            if H % ws != 0:\n",
    "                HP = int((H + ws - 1) / ws) * ws\n",
    "                qh = (\n",
    "                    F.pad(\n",
    "                        qh.transpose(-1, -2).reshape(B, -1, H, W),\n",
    "                        pad=[0, 0, 0, HP - H],\n",
    "                    )\n",
    "                    .flatten(2, 3)\n",
    "                    .transpose(-1, -2)\n",
    "                )\n",
    "                vh = (\n",
    "                    F.pad(\n",
    "                        vh.transpose(-1, -2).reshape(B, -1, H, W),\n",
    "                        pad=[0, 0, 0, HP - H],\n",
    "                    )\n",
    "                    .flatten(2, 3)\n",
    "                    .transpose(-1, -2)\n",
    "                )\n",
    "                kh = vh\n",
    "            else:\n",
    "                HP = H\n",
    "\n",
    "            if W % ws != 0:\n",
    "                WP = int((W + ws - 1) / ws) * ws\n",
    "                qv = (\n",
    "                    F.pad(\n",
    "                        qv.transpose(-1, -2).reshape(B, -1, H, W),\n",
    "                        pad=[0, WP - W, 0, 0],\n",
    "                    )\n",
    "                    .flatten(2, 3)\n",
    "                    .transpose(-1, -2)\n",
    "                )\n",
    "                vv = (\n",
    "                    F.pad(\n",
    "                        vv.transpose(-1, -2).reshape(B, -1, H, W),\n",
    "                        pad=[0, WP - W, 0, 0],\n",
    "                    )\n",
    "                    .flatten(2, 3)\n",
    "                    .transpose(-1, -2)\n",
    "                )\n",
    "                kv = vv\n",
    "            else:\n",
    "                WP = W\n",
    "\n",
    "            attn_h = self._cross_shaped_attention(\n",
    "                qh,\n",
    "                kh,\n",
    "                vh,\n",
    "                H,\n",
    "                W,\n",
    "                HP,\n",
    "                W,\n",
    "                ws,\n",
    "                horizontal=True,\n",
    "            )\n",
    "            attn_v = self._cross_shaped_attention(\n",
    "                qv,\n",
    "                kv,\n",
    "                vv,\n",
    "                H,\n",
    "                W,\n",
    "                H,\n",
    "                WP,\n",
    "                ws,\n",
    "                horizontal=False,\n",
    "            )\n",
    "\n",
    "            attn = torch.cat([attn_h, attn_v], dim=-1)\n",
    "            attn = attn.add(v_conv)\n",
    "            attn = self.attn_act(attn)\n",
    "\n",
    "            attn = self.to_out(attn)\n",
    "            attn = self.attn_bn(attn.flatten(0, 1)).view_as(attn)\n",
    "            return attn\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            x = cp.checkpoint(_inner_forward, x)\n",
    "        else:\n",
    "            x = _inner_forward(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        s = f\"window_size={self.ws}\"\n",
    "        return s\n",
    "\n",
    "\n",
    "class HRViTBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int = 64,\n",
    "        dim: int = 64,\n",
    "        heads: int = 2,\n",
    "        proj_dropout: float = 0.0,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        drop_path: float = 0.0,\n",
    "        ws: int = 1,\n",
    "        with_cp: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.with_cp = with_cp\n",
    "\n",
    "        # build layer normalization\n",
    "        self.attn_norm = nn.LayerNorm(in_dim)\n",
    "\n",
    "        # build attention layer\n",
    "        self.attn = HRViTAttention(\n",
    "            in_dim=in_dim,\n",
    "            dim=dim,\n",
    "            heads=heads,\n",
    "            ws=ws,\n",
    "            proj_drop=proj_dropout,\n",
    "            with_cp=with_cp,\n",
    "        )\n",
    "\n",
    "        # build diversity-enhanced shortcut DES\n",
    "        self.des = DES(\n",
    "            in_features=in_dim,\n",
    "            out_features=dim,\n",
    "            bias=True,\n",
    "            act_func=nn.GELU,\n",
    "        )\n",
    "        # build drop path\n",
    "        self.attn_drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "        # build layer normalization\n",
    "        self.ffn_norm = nn.LayerNorm(in_dim)\n",
    "\n",
    "        # build FFN\n",
    "        self.ffn = MixCFN(\n",
    "            in_features=in_dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            out_features=dim,\n",
    "            act_func=nn.GELU,\n",
    "            with_cp=with_cp,\n",
    "        )\n",
    "\n",
    "        # build drop path\n",
    "        self.ffn_drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x: Tensor, H: int, W: int) -> Tensor:\n",
    "        # attention block\n",
    "        res = x\n",
    "        x = self.attn_norm(x)\n",
    "        x = self.attn(x, H, W)\n",
    "        x_des = self.des(res)\n",
    "        x = self.attn_drop_path(x.add(x_des)).add(res)\n",
    "\n",
    "        # ffn block\n",
    "        res = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x, H, W)\n",
    "        x = self.ffn_drop_path(x).add(res)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class HRViTPatchEmbed(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        patch_size: _size = 3,\n",
    "        stride: int = 1,\n",
    "        dim: int = 64,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_size = to_2tuple(patch_size)\n",
    "        self.dim = dim\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                dim,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                dim,\n",
    "                dim,\n",
    "                kernel_size=self.patch_size,\n",
    "                stride=stride,\n",
    "                padding=(self.patch_size[0] // 2, self.patch_size[1] // 2),\n",
    "                groups=dim,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, int, int]:\n",
    "        x = self.proj(x)\n",
    "        H, W = x.shape[-2:]\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e04f8da-95da-4992-9b83-0af3449688ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRViTStage(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        #### Patch Embed Config ####\n",
    "        in_channels: Tuple[\n",
    "            int,\n",
    "        ] = (32, 64, 128, 256),\n",
    "        out_channels: Tuple[\n",
    "            int,\n",
    "        ] = (32, 64, 128, 256),\n",
    "        block_list: Tuple[\n",
    "            int,\n",
    "        ] = (1, 1, 6, 2),\n",
    "        #### HRViTAttention Config ####\n",
    "        dim_head: int = 32,\n",
    "        ws_list: Tuple[\n",
    "            int,\n",
    "        ] = (1, 2, 7, 7),\n",
    "        proj_dropout: float = 0.0,\n",
    "        drop_path_rates: Tuple[float] = (\n",
    "            0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "        ),  # different droprate for different attn/mlp\n",
    "        #### MixCFN Config ####\n",
    "        mlp_ratio_list: Tuple[\n",
    "            int,\n",
    "        ] = (4, 4, 4, 4),\n",
    "        dropout: float = 0.0,\n",
    "        #### Gradient Checkpointing #####\n",
    "        with_cp: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.ModuleList(\n",
    "            [\n",
    "                HRViTPatchEmbed(\n",
    "                    in_channels=inc,\n",
    "                    patch_size=3,\n",
    "                    stride=1,\n",
    "                    dim=outc,\n",
    "                )\n",
    "                for inc, outc in zip(in_channels, out_channels)\n",
    "            ]\n",
    "        )  # one patch embedding for each branch\n",
    "\n",
    "        ## we arrange blocks in stages/layers\n",
    "        n_inputs = len(out_channels)\n",
    "\n",
    "        self.branches = nn.ModuleList([])\n",
    "        for i, n_blocks in enumerate(block_list[:n_inputs]):\n",
    "            blocks = []\n",
    "            for j in range(n_blocks):\n",
    "                blocks += [\n",
    "                    HRViTBlock(\n",
    "                        in_dim=out_channels[i],\n",
    "                        dim=out_channels[i],\n",
    "                        heads=out_channels[i] // dim_head,  # automatically derive heads\n",
    "                        proj_dropout=proj_dropout,\n",
    "                        mlp_ratio=mlp_ratio_list[i],\n",
    "                        drop_path=drop_path_rates[j],\n",
    "                        ws=ws_list[i],\n",
    "                        with_cp=with_cp,\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "            blocks = nn.ModuleList(blocks)\n",
    "            self.branches.append(blocks)\n",
    "        self.norm = nn.ModuleList([nn.LayerNorm(outc) for outc in out_channels])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tuple[\n",
    "            Tensor,\n",
    "        ],\n",
    "    ) -> Tuple[Tensor,]:\n",
    "        B = x[0].shape[0]\n",
    "        x = list(x)\n",
    "        H, W = [], []\n",
    "        ## patch embed\n",
    "        for i, (xx, embed) in enumerate(zip(x, self.patch_embed)):\n",
    "            xx, h, w = embed(xx)\n",
    "            print(xx.shape)\n",
    "            x[i] = xx\n",
    "            H.append(h)\n",
    "            W.append(w)\n",
    "\n",
    "        ## HRViT blocks\n",
    "        for i, (branch, h, w) in enumerate(zip(self.branches, H, W)):\n",
    "            for block in branch:\n",
    "                x[i] = block(x[i], h, w)\n",
    "\n",
    "        ## LN at the end of each stage\n",
    "        for i, (xx, norm, h, w) in enumerate(zip(x, self.norm, H, W)):\n",
    "            xx = norm(xx)\n",
    "            xx = xx.reshape(B, h, w, -1).permute(0, 3, 1, 2).contiguous()\n",
    "            x[i] = xx\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e5984d-92dc-4a8b-af53-5e2fb05a3885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4096, 32])\n",
      "torch.Size([2, 4096, 64])\n",
      "torch.Size([2, 4096, 128])\n",
      "torch.Size([2, 4096, 256])\n",
      "Output 1: torch.Size([2, 32, 64, 64])\n",
      "Output 2: torch.Size([2, 64, 64, 64])\n",
      "Output 3: torch.Size([2, 128, 64, 64])\n",
      "Output 4: torch.Size([2, 256, 64, 64])\n",
      "HRViTStage(\n",
      "  (patch_embed): ModuleList(\n",
      "    (0): HRViTPatchEmbed(\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      )\n",
      "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): HRViTPatchEmbed(\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): HRViTPatchEmbed(\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): HRViTPatchEmbed(\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (branches): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): HRViTBlock(\n",
      "        (attn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): HRViTAttention(\n",
      "          window_size=1\n",
      "          (to_qkv): Linear(in_features=32, out_features=64, bias=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (attn_act): Hardswish()\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (attn_bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (parallel_conv): Sequential(\n",
      "            (0): Hardswish()\n",
      "            (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "          )\n",
      "        )\n",
      "        (des): DES(\n",
      "          (proj_right): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (proj_left): Linear(in_features=4, out_features=4, bias=True)\n",
      "        )\n",
      "        (attn_drop_path): Identity()\n",
      "        (ffn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): MixCFN(\n",
      "          (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (conv): MixConv2d(\n",
      "            (conv_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "            (conv_5): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        )\n",
      "        (ffn_drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): HRViTBlock(\n",
      "        (attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): HRViTAttention(\n",
      "          window_size=2\n",
      "          (to_qkv): Linear(in_features=64, out_features=128, bias=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (attn_act): Hardswish()\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (attn_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (parallel_conv): Sequential(\n",
      "            (0): Hardswish()\n",
      "            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          )\n",
      "        )\n",
      "        (des): DES(\n",
      "          (proj_right): Linear(in_features=8, out_features=8, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (proj_left): Linear(in_features=8, out_features=8, bias=True)\n",
      "        )\n",
      "        (attn_drop_path): Identity()\n",
      "        (ffn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): MixCFN(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (conv): MixConv2d(\n",
      "            (conv_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "            (conv_5): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "        (ffn_drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0-5): 6 x HRViTBlock(\n",
      "        (attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): HRViTAttention(\n",
      "          window_size=7\n",
      "          (to_qkv): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (attn_act): Hardswish()\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (attn_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (parallel_conv): Sequential(\n",
      "            (0): Hardswish()\n",
      "            (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          )\n",
      "        )\n",
      "        (des): DES(\n",
      "          (proj_right): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (proj_left): Linear(in_features=8, out_features=8, bias=True)\n",
      "        )\n",
      "        (attn_drop_path): Identity()\n",
      "        (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): MixCFN(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (conv): MixConv2d(\n",
      "            (conv_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (conv_5): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (ffn_drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0-1): 2 x HRViTBlock(\n",
      "        (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): HRViTAttention(\n",
      "          window_size=7\n",
      "          (to_qkv): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (attend): Softmax(dim=-1)\n",
      "          (attn_act): Hardswish()\n",
      "          (to_out): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (attn_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (parallel_conv): Sequential(\n",
      "            (0): Hardswish()\n",
      "            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "        )\n",
      "        (des): DES(\n",
      "          (proj_right): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (proj_left): Linear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (attn_drop_path): Identity()\n",
      "        (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): MixCFN(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (conv): MixConv2d(\n",
      "            (conv_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "            (conv_5): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)\n",
      "          )\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (ffn_drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): ModuleList(\n",
      "    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = HRViTPatchEmbed(\n",
    "                    in_channels=32,\n",
    "                    patch_size=16,\n",
    "                    stride=16,\n",
    "                    dim=32,\n",
    "                )\n",
    "\n",
    "    x = torch.randn(2, 32, 64, 64)\n",
    "\n",
    "    output, h, w = model(x)\n",
    "    \n",
    "    stage = HRViTStage()\n",
    "    batch_size = 2\n",
    "    input_shapes = [(32, 64, 64), (64, 64, 64), (128, 64, 64), (256, 64, 64)]\n",
    "    sample_input = [torch.randn(batch_size, *shape) for shape in input_shapes]\n",
    "\n",
    "    output = stage(sample_input)\n",
    "    \n",
    "    for i, out in enumerate(output):\n",
    "        print(f\"Output {i+1}: {out.shape}\")\n",
    "\n",
    "    print(stage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plant)",
   "language": "python",
   "name": "plant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
